from pyspark.sql.functions import col, rand
df = (spark.range(1, 1000000)
      .withColumn("id", (col("id") / 1000).cast("integer"))
      .withColumn("v", rand(seed=1)))
display(df.sample(.001))
df.createOrReplaceTempView("df_temp")
df.count()
df.rdd.getNumPartitions()
df.cache().count()
df.count()
df.limit(10).toPandas()
