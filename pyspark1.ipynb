{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/config/workspace'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 281.4 MB 19 kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting py4j==0.10.9.5\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "\u001b[K     |████████████████████████████████| 199 kB 68.4 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824021 sha256=5602e3f05e461ed308fc2cbca8ddad055ba99771f0cc39d433cacfcdf02836d0\n",
      "  Stored in directory: /config/.cache/pip/wheels/b1/59/a0/a1a0624b5e865fd389919c1a10f53aec9b12195d6747710baf\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/11 07:47:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"spark://localhost:7077\").appName(\"demo1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('myfile','rw') as file:\n",
    "    for line in file:\n",
    "        if not line.isspace():\n",
    "            file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from pyspark.sql.functions import col, rand\n",
      "\n",
      "df = (spark.range(1, 1000000)\n",
      "\n",
      "      .withColumn(\"id\", (col(\"id\") / 1000).cast(\"integer\"))\n",
      "\n",
      "      .withColumn(\"v\", rand(seed=1)))\n",
      "\n",
      "display(df.sample(.001))\n",
      "\n",
      "df.createOrReplaceTempView(\"df_temp\")\n",
      "\n",
      "df.count()\n",
      "\n",
      "df.rdd.getNumPartitions()\n",
      "\n",
      "df.cache().count()\n",
      "\n",
      "df.count()\n",
      "\n",
      "df.limit(10).toPandas()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# defining object file1 to\n",
    "# open GeeksforGeeks file in\n",
    "# read mode\n",
    "file1 = open('GeeksforGeeks.txt',\n",
    "\t\t\t'r')\n",
    "\n",
    "# defining object file2 to\n",
    "# open GeeksforGeeksUpdated file\n",
    "# in write mode\n",
    "file2 = open('GeeksforGeeksUpdated.txt',\n",
    "\t\t\t'w')\n",
    "\n",
    "# reading each line from original\n",
    "# text file\n",
    "for line in file1.readlines():\n",
    "\n",
    "\t# reading all lines that do not\n",
    "\t# begin with \"TextGenerator\"\n",
    "\tif not (line.startswith('#')):\n",
    "\t\tif not line.isspace():\n",
    "\t\n",
    "\t\t# printing those lines\n",
    "\t\t\tprint(line)\n",
    "\t\t\n",
    "\t\t# storing only those lines that\n",
    "\t\t# do not begin with \"TextGenerator\"\n",
    "\t\t\tfile2.write(line)\n",
    "\n",
    "# close and save the files\n",
    "file2.close()\n",
    "file1.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
